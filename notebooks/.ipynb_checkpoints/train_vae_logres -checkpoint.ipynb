{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.layers import Dense, Input, Lambda, BatchNormalization, Activation\n",
    "from keras.losses import mse, binary_crossentropy, kullback_leibler_divergence\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "X_brca_train = pd.read_csv(\"data/ciriello_brca_filtered.csv\")\n",
    "X_brca_train = X_brca_train[X_brca_train.Ciriello_subtype != \"Normal\"]\n",
    "\n",
    "X_train.drop(['Ciriello_subtype'], axis=\"columns\", inplace=True)\n",
    "\n",
    "X_tcga_no_brca = pd.read_csv(\"data/tcga_filtered_no_brca.csv\")\n",
    "\n",
    "# Test data\n",
    "X_brca_test = pd.read_csv(\"data/brca_final_test.csv\")\n",
    "X_brca_test = X_brca_test[X_brca_test.subtype != \"Normal\"]\n",
    "y_test = X_brca_test[\"subtype\"]\n",
    "\n",
    "X_brca_test.drop(['subtype'], axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and normalize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristovao/miniconda3/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "y = X_brca[\"subtype\"]\n",
    "X_brca_train, X_brca_test = train_test_split(X_brca, stratify=y, test_size=0.2)\n",
    "\n",
    "y_brca_train = X_brca_train[\"subtype\"]\n",
    "y_brca_test = X_brca_test[\"subtype\"]\n",
    "\n",
    "X_brca_train.drop(['subtype'], axis=\"columns\", inplace=True)\n",
    "X_brca_test.drop(['subtype'], axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variational Autoencoder auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    \n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim), mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def vae_loss(y_true, y_pred):\n",
    "    # E[log P(X|z)]\n",
    "    reconstruction_loss = original_dim * binary_crossentropy(y_true, y_pred) # because it returns the mean cross-entropy\n",
    "    # reconstruction_loss = mse(y_true, y_pred)\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "    kl_loss = -0.5 * K.sum(1. + z_log_var_encoded - K.exp(z_log_var_encoded) - K.square(z_mean_encoded), axis=1)\n",
    "\n",
    "    return K.mean(reconstruction_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_autoencoder, X_train_logreg, y_train_logreg, X_val_logreg, y_val_logreg, intermediate_dim=100, latent_dim=100):\n",
    "    \n",
    "    #Initialize variables and hyperparameters\n",
    "    input_shape = (original_dim,)\n",
    "    intermediate_dim = intermediate_dim\n",
    "    latent_dim = latent_dim\n",
    "\n",
    "    batch_size = 100\n",
    "    epochs = 150 \n",
    "    learning_rate = 0.001\n",
    "\n",
    "    #Implement Variational Autoencoder\n",
    "    \n",
    "    # Build Encoder\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    hidden_dense = Dense(intermediate_dim)(inputs)\n",
    "    hidden_dense_batchnorm = BatchNormalization()(hidden_dense)\n",
    "    hidden_dense_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "    z_mean_dense = Dense(latent_dim, name='z_mean')(hidden_dense_encoded)\n",
    "    z_log_var_dense = Dense(latent_dim, name='z_log_var')(hidden_dense_encoded)\n",
    "    \n",
    "    z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense)\n",
    "    z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "    \n",
    "    z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense)\n",
    "    z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "    encoder = Model(inputs, [z_mean_encoded, z_log_var_encoded, z], name='encoder')\n",
    "    \n",
    "    # Build Decoder\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    decoder_hidden = Dense(intermediate_dim, activation='relu', name='decoder_hidden')(latent_inputs)\n",
    "    outputs = Dense(original_dim, activation='sigmoid', name='decoder_output')(decoder_hidden)\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    \n",
    "    #Build Variational Autoencoder\n",
    "    outputs = decoder(encoder(inputs)[2]) # fetches the z layer, the sampled one\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "    adam = optimizers.Adam(lr=learning_rate)\n",
    "    \n",
    "    def vae_loss(y_true, y_pred):\n",
    "        # E[log P(X|z)]\n",
    "        reconstruction_loss = original_dim * binary_crossentropy(y_true, y_pred) # because it returns the mean cross-entropy\n",
    "        # reconstruction_loss = mse(y_true, y_pred)\n",
    "        # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var_encoded - K.exp(z_log_var_encoded) - K.square(z_mean_encoded), axis=1)\n",
    "\n",
    "        return K.mean(reconstruction_loss + kl_loss)\n",
    "    \n",
    "    vae.compile(optimizer=adam, loss=vae_loss)\n",
    "    \n",
    "    # Split validation set\n",
    "    test_set_percent = 0.1\n",
    "    X_autoencoder_val = X_autoencoder.sample(frac=test_set_percent)\n",
    "    X_autoencoder_train = X_autoencoder.drop(X_autoencoder_val.index)\n",
    "    \n",
    "    # Limit number of CPU cores used\n",
    "    K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10)))\n",
    "    \n",
    "    # Train the model\n",
    "    \n",
    "    fit_hist = vae.fit(X_autoencoder_train, \n",
    "                    X_autoencoder_train,\n",
    "                    shuffle=True,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)],\n",
    "                    validation_data=(X_autoencoder_val, X_autoencoder_val))\n",
    "    \n",
    "    encoded_train_logreg = encoder.predict(X_train_logreg)\n",
    "    encoded_train_logreg = pd.DataFrame(encoded_train_logreg[0])\n",
    "\n",
    "    encoded_val_logreg = encoder.predict(X_val_logreg)\n",
    "    encoded_val_logreg = pd.DataFrame(encoded_val_logreg[0])\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear', penalty=\"l1\", C=1, multi_class=\"auto\").fit(encoded_train_logreg, y_train_logreg)\n",
    "    \n",
    "    history_df = pd.DataFrame(fit_hist.history)\n",
    "    score = clf.score(encoded_val_logreg, y_val_logreg)\n",
    "    conf_matrix = confusion_matrix(y_val_logreg, clf.predict(encoded_val_data))\n",
    "    \n",
    "    return history_df, score, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for the whole model - 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 of 10\n",
      "Train on 9995 samples, validate on 1110 samples\n",
      "Epoch 1/150\n",
      " - 27s - loss: -1.1952e+04 - val_loss: -3.3035e+04\n",
      "Epoch 2/150\n",
      " - 23s - loss: -4.2624e+04 - val_loss: -4.9090e+04\n",
      "Epoch 3/150\n",
      " - 23s - loss: -5.2200e+04 - val_loss: -5.4958e+04\n",
      "Epoch 4/150\n",
      " - 23s - loss: -5.8301e+04 - val_loss: -6.0875e+04\n",
      "Epoch 5/150\n",
      " - 22s - loss: -6.3745e+04 - val_loss: -6.5509e+04\n",
      "Epoch 6/150\n",
      " - 22s - loss: -6.7684e+04 - val_loss: -6.8716e+04\n",
      "Epoch 7/150\n",
      " - 22s - loss: -7.0439e+04 - val_loss: -7.1240e+04\n",
      "Epoch 8/150\n",
      " - 22s - loss: -7.2915e+04 - val_loss: -7.3584e+04\n",
      "Epoch 9/150\n",
      " - 22s - loss: -7.4968e+04 - val_loss: -7.5633e+04\n",
      "Epoch 10/150\n",
      " - 22s - loss: -7.6886e+04 - val_loss: -7.7440e+04\n",
      "Epoch 11/150\n",
      " - 22s - loss: -7.8446e+04 - val_loss: -7.8838e+04\n",
      "Epoch 12/150\n",
      " - 22s - loss: -7.9764e+04 - val_loss: -8.0073e+04\n",
      "Epoch 13/150\n",
      " - 23s - loss: -8.0886e+04 - val_loss: -8.1165e+04\n",
      "Epoch 14/150\n",
      " - 22s - loss: -8.1878e+04 - val_loss: -8.2110e+04\n",
      "Epoch 15/150\n",
      " - 22s - loss: -8.2773e+04 - val_loss: -8.2791e+04\n",
      "Epoch 16/150\n",
      " - 22s - loss: -8.3479e+04 - val_loss: -8.3634e+04\n",
      "Epoch 17/150\n",
      " - 22s - loss: -8.4265e+04 - val_loss: -8.4274e+04\n",
      "Epoch 18/150\n",
      " - 22s - loss: -8.4850e+04 - val_loss: -8.4836e+04\n",
      "Epoch 19/150\n",
      " - 22s - loss: -8.5429e+04 - val_loss: -8.5370e+04\n",
      "Epoch 20/150\n",
      " - 22s - loss: -8.5974e+04 - val_loss: -8.5813e+04\n",
      "Epoch 21/150\n",
      " - 22s - loss: -8.6469e+04 - val_loss: -8.6336e+04\n",
      "Epoch 22/150\n",
      " - 22s - loss: -8.6912e+04 - val_loss: -8.6722e+04\n",
      "Epoch 23/150\n",
      " - 22s - loss: -8.7289e+04 - val_loss: -8.7047e+04\n",
      "Epoch 24/150\n",
      " - 22s - loss: -8.7611e+04 - val_loss: -8.7353e+04\n",
      "Epoch 25/150\n",
      " - 22s - loss: -8.7996e+04 - val_loss: -8.7751e+04\n",
      "Epoch 26/150\n",
      " - 22s - loss: -8.8372e+04 - val_loss: -8.8017e+04\n",
      "Epoch 27/150\n",
      " - 22s - loss: -8.8618e+04 - val_loss: -8.8350e+04\n",
      "Epoch 28/150\n",
      " - 22s - loss: -8.8934e+04 - val_loss: -8.8596e+04\n",
      "Epoch 29/150\n",
      " - 22s - loss: -8.9235e+04 - val_loss: -8.8806e+04\n",
      "Epoch 30/150\n",
      " - 22s - loss: -8.9434e+04 - val_loss: -8.9022e+04\n",
      "Epoch 31/150\n",
      " - 22s - loss: -8.9671e+04 - val_loss: -8.9198e+04\n",
      "Epoch 32/150\n",
      " - 22s - loss: -8.9884e+04 - val_loss: -8.9393e+04\n",
      "Epoch 33/150\n",
      " - 22s - loss: -9.0104e+04 - val_loss: -8.9634e+04\n",
      "Epoch 34/150\n",
      " - 22s - loss: -9.0354e+04 - val_loss: -8.9796e+04\n",
      "Epoch 35/150\n",
      " - 22s - loss: -9.0481e+04 - val_loss: -9.0053e+04\n",
      "Epoch 36/150\n",
      " - 22s - loss: -9.0714e+04 - val_loss: -9.0253e+04\n",
      "Epoch 37/150\n",
      " - 22s - loss: -9.0887e+04 - val_loss: -9.0393e+04\n",
      "Epoch 38/150\n",
      " - 22s - loss: -9.1056e+04 - val_loss: -9.0485e+04\n",
      "Epoch 39/150\n",
      " - 22s - loss: -9.1264e+04 - val_loss: -9.0675e+04\n",
      "Epoch 40/150\n",
      " - 22s - loss: -9.1413e+04 - val_loss: -9.0835e+04\n",
      "Epoch 41/150\n",
      " - 22s - loss: -9.1600e+04 - val_loss: -9.0930e+04\n",
      "Epoch 42/150\n",
      " - 22s - loss: -9.1702e+04 - val_loss: -9.1065e+04\n",
      "Epoch 43/150\n",
      " - 22s - loss: -9.1864e+04 - val_loss: -9.1205e+04\n",
      "Epoch 44/150\n",
      " - 22s - loss: -9.1993e+04 - val_loss: -9.1396e+04\n",
      "Epoch 45/150\n",
      " - 22s - loss: -9.2121e+04 - val_loss: -9.1450e+04\n",
      "Epoch 46/150\n",
      " - 22s - loss: -9.2245e+04 - val_loss: -9.1671e+04\n",
      "Epoch 47/150\n",
      " - 22s - loss: -9.2413e+04 - val_loss: -9.1706e+04\n",
      "Epoch 48/150\n",
      " - 22s - loss: -9.2535e+04 - val_loss: -9.1842e+04\n",
      "Epoch 49/150\n",
      " - 22s - loss: -9.2627e+04 - val_loss: -9.1856e+04\n",
      "Epoch 50/150\n",
      " - 22s - loss: -9.2716e+04 - val_loss: -9.2043e+04\n",
      "Epoch 51/150\n",
      " - 22s - loss: -9.2826e+04 - val_loss: -9.2104e+04\n",
      "Epoch 52/150\n",
      " - 22s - loss: -9.2962e+04 - val_loss: -9.2244e+04\n",
      "Epoch 53/150\n",
      " - 22s - loss: -9.3087e+04 - val_loss: -9.2381e+04\n",
      "Epoch 54/150\n",
      " - 22s - loss: -9.3188e+04 - val_loss: -9.2379e+04\n",
      "Epoch 55/150\n",
      " - 22s - loss: -9.3280e+04 - val_loss: -9.2485e+04\n",
      "Epoch 56/150\n",
      " - 22s - loss: -9.3406e+04 - val_loss: -9.2543e+04\n",
      "Epoch 57/150\n",
      " - 22s - loss: -9.3458e+04 - val_loss: -9.2716e+04\n",
      "Epoch 58/150\n",
      " - 22s - loss: -9.3587e+04 - val_loss: -9.2678e+04\n",
      "Epoch 59/150\n",
      " - 22s - loss: -9.3640e+04 - val_loss: -9.2856e+04\n",
      "Epoch 60/150\n",
      " - 22s - loss: -9.3729e+04 - val_loss: -9.2937e+04\n",
      "Epoch 61/150\n",
      " - 22s - loss: -9.3853e+04 - val_loss: -9.2893e+04\n",
      "Epoch 62/150\n",
      " - 22s - loss: -9.3904e+04 - val_loss: -9.3012e+04\n",
      "Epoch 63/150\n",
      " - 21s - loss: -9.3918e+04 - val_loss: -9.3132e+04\n",
      "Epoch 64/150\n",
      " - 22s - loss: -9.4044e+04 - val_loss: -9.3152e+04\n",
      "Epoch 65/150\n",
      " - 22s - loss: -9.4190e+04 - val_loss: -9.3274e+04\n",
      "Epoch 66/150\n",
      " - 22s - loss: -9.4195e+04 - val_loss: -9.3297e+04\n",
      "Epoch 67/150\n",
      " - 22s - loss: -9.4327e+04 - val_loss: -9.3464e+04\n",
      "Epoch 68/150\n",
      " - 21s - loss: -9.4355e+04 - val_loss: -9.3467e+04\n",
      "Epoch 69/150\n",
      " - 22s - loss: -9.4444e+04 - val_loss: -9.3391e+04\n",
      "Epoch 70/150\n",
      " - 21s - loss: -9.4501e+04 - val_loss: -9.3529e+04\n",
      "Epoch 71/150\n",
      " - 21s - loss: -9.4607e+04 - val_loss: -9.3572e+04\n",
      "Epoch 72/150\n",
      " - 21s - loss: -9.4635e+04 - val_loss: -9.3669e+04\n",
      "Epoch 73/150\n",
      " - 21s - loss: -9.4752e+04 - val_loss: -9.3709e+04\n",
      "Epoch 74/150\n",
      " - 21s - loss: -9.4819e+04 - val_loss: -9.3871e+04\n",
      "Epoch 75/150\n",
      " - 21s - loss: -9.4819e+04 - val_loss: -9.3878e+04\n",
      "Epoch 76/150\n",
      " - 21s - loss: -9.4837e+04 - val_loss: -9.3840e+04\n",
      "Epoch 77/150\n",
      " - 21s - loss: -9.4950e+04 - val_loss: -9.3911e+04\n",
      "Epoch 78/150\n",
      " - 21s - loss: -9.5038e+04 - val_loss: -9.3979e+04\n",
      "Epoch 79/150\n",
      " - 21s - loss: -9.5061e+04 - val_loss: -9.4044e+04\n",
      "Epoch 80/150\n",
      " - 22s - loss: -9.5146e+04 - val_loss: -9.4145e+04\n",
      "Epoch 81/150\n",
      " - 21s - loss: -9.5153e+04 - val_loss: -9.4159e+04\n",
      "Epoch 82/150\n",
      " - 21s - loss: -9.5280e+04 - val_loss: -9.4196e+04\n",
      "Epoch 83/150\n",
      " - 21s - loss: -9.5290e+04 - val_loss: -9.4184e+04\n",
      "Epoch 84/150\n",
      " - 21s - loss: -9.5369e+04 - val_loss: -9.4307e+04\n",
      "Epoch 85/150\n",
      " - 21s - loss: -9.5380e+04 - val_loss: -9.4302e+04\n",
      "Epoch 86/150\n",
      " - 21s - loss: -9.5447e+04 - val_loss: -9.4414e+04\n",
      "Epoch 87/150\n",
      " - 21s - loss: -9.5518e+04 - val_loss: -9.4427e+04\n",
      "Epoch 88/150\n"
     ]
    }
   ],
   "source": [
    "history = {}\n",
    "scores = []\n",
    "confusion_matrixes = []\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "i=1\n",
    "\n",
    "for train_index, test_index in skf.split(X_brca_train, y_brca_train):\n",
    "    print('Fold {} of {}'.format(i, skf.n_splits))\n",
    "    \n",
    "    X_train, X_val = X_brca_train.iloc[train_index], X_brca_train.iloc[test_index]\n",
    "    y_train, y_val = y_brca_train.iloc[train_index], y_brca_train.iloc[test_index]\n",
    "    \n",
    "    # Prepare data to train Variational Autoencoder (merge dataframes and normalize)\n",
    "    X_autoencoder = pd.concat([X_train, X_tcga_no_brca], sort=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_autoencoder)\n",
    "    X_autoencoder_scaled = pd.DataFrame(scaler.transform(X_autoencoder), columns=X_autoencoder.columns)\n",
    "\n",
    "    # Scale logistic regression data\n",
    "    scaler.fit(X_brca_train)\n",
    "    X_brca_train_scaled = pd.DataFrame(scaler.transform(X_brca_train), columns=X_brca_train.columns)\n",
    "    X_brca_test_scaled = pd.DataFrame(scaler.transform(X_brca_test), columns=X_brca_test.columns)\n",
    "    \n",
    "    \n",
    "    # Because it is used by the error function\n",
    "    original_dim = X_autoencoder.shape[1]\n",
    "    \n",
    "    #Train the Model\n",
    "    hist, score, conf_matrix = train_model(X_autoencoder=X_autoencoder_scaled, \n",
    "                                           X_train_logreg=X_train, \n",
    "                                           y_train_logreg=y_train, \n",
    "                                           X_val_logreg=X_val, \n",
    "                                           y_val_logreg=y_val,\n",
    "                                           intermediate_dim=100,\n",
    "                                           latent_dim=100)\n",
    "    \n",
    "    print(score)\n",
    "    history[i-1] = hist\n",
    "    scores.append(score)\n",
    "    confusion_matrixes.append(conf_matrix)\n",
    "    i+=1\n",
    "print('10-Fold results: {}'.format(scores))\n",
    "print('Latent dim: {}, Accuracy: {}'.format(latent_dim, np.mean(scores)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
