{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.layers import Dense, Input, Lambda, BatchNormalization, Activation\n",
    "from keras.losses import mse, binary_crossentropy, kullback_leibler_divergence\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "X_brca_train = pd.read_csv(\"data/ciriello_brca_filtered_train.csv\")\n",
    "X_brca_train = X_brca_train[X_brca_train.Ciriello_subtype != \"Normal\"]\n",
    "\n",
    "y_brca_train = X_brca_train[\"Ciriello_subtype\"]\n",
    "\n",
    "X_brca_train.drop(['Ciriello_subtype'], axis=\"columns\", inplace=True)\n",
    "\n",
    "X_tcga_no_brca = pd.read_csv(\"data/tcga_filtered_no_brca.csv\")\n",
    "\n",
    "# Test data\n",
    "X_brca_test = pd.read_csv(\"data/tcga_brca_filtered_test.csv\")\n",
    "X_brca_test = X_brca_test[X_brca_test.subtype != \"Normal\"]\n",
    "y_brca_test = X_brca_test[\"subtype\"]\n",
    "\n",
    "X_brca_test.drop(['subtype'], axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variational Autoencoder auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    \n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim), mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def vae_loss(y_true, y_pred):\n",
    "    # E[log P(X|z)]\n",
    "    reconstruction_loss = original_dim * binary_crossentropy(y_true, y_pred) # because it returns the mean cross-entropy\n",
    "    # reconstruction_loss = mse(y_true, y_pred)\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "    kl_loss = -0.5 * K.sum(1. + z_log_var_encoded - K.exp(z_log_var_encoded) - K.square(z_mean_encoded), axis=1)\n",
    "\n",
    "    return K.mean(reconstruction_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_autoencoder, X_train_logreg, y_train_logreg, X_val_logreg, y_val_logreg, intermediate_dim=100, latent_dim=100):\n",
    "    \n",
    "    #Initialize variables and hyperparameters\n",
    "    input_shape = (original_dim,)\n",
    "    intermediate_dim = intermediate_dim\n",
    "    latent_dim = latent_dim\n",
    "\n",
    "    batch_size = 100\n",
    "    epochs = 150 \n",
    "    learning_rate = 0.001\n",
    "\n",
    "    #Implement Variational Autoencoder\n",
    "    \n",
    "    # Build Encoder\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    hidden_dense = Dense(intermediate_dim)(inputs)\n",
    "    hidden_dense_batchnorm = BatchNormalization()(hidden_dense)\n",
    "    hidden_dense_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "    z_mean_dense = Dense(latent_dim, name='z_mean')(hidden_dense_encoded)\n",
    "    z_log_var_dense = Dense(latent_dim, name='z_log_var')(hidden_dense_encoded)\n",
    "    \n",
    "    z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense)\n",
    "    z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "    \n",
    "    z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense)\n",
    "    z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "    encoder = Model(inputs, [z_mean_encoded, z_log_var_encoded, z], name='encoder')\n",
    "    \n",
    "    # Build Decoder\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    decoder_hidden = Dense(intermediate_dim, activation='relu', name='decoder_hidden')(latent_inputs)\n",
    "    outputs = Dense(original_dim, activation='sigmoid', name='decoder_output')(decoder_hidden)\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    \n",
    "    #Build Variational Autoencoder\n",
    "    outputs = decoder(encoder(inputs)[2]) # fetches the z layer, the sampled one\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "    adam = optimizers.Adam(lr=learning_rate)\n",
    "    \n",
    "    def vae_loss(y_true, y_pred):\n",
    "        # E[log P(X|z)]\n",
    "        reconstruction_loss = original_dim * binary_crossentropy(y_true, y_pred) # because it returns the mean cross-entropy\n",
    "        # reconstruction_loss = mse(y_true, y_pred)\n",
    "        # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var_encoded - K.exp(z_log_var_encoded) - K.square(z_mean_encoded), axis=1)\n",
    "\n",
    "        return K.mean(reconstruction_loss + kl_loss)\n",
    "    \n",
    "    vae.compile(optimizer=adam, loss=vae_loss)\n",
    "    \n",
    "    # Split validation set\n",
    "    test_set_percent = 0.1\n",
    "    X_autoencoder_val = X_autoencoder.sample(frac=test_set_percent)\n",
    "    X_autoencoder_train = X_autoencoder.drop(X_autoencoder_val.index)\n",
    "    \n",
    "    # Limit number of CPU cores used\n",
    "    K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10)))\n",
    "    \n",
    "    # Train the model\n",
    "    \n",
    "    fit_hist = vae.fit(X_autoencoder_train, \n",
    "                    X_autoencoder_train,\n",
    "                    shuffle=True,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)],\n",
    "                    validation_data=(X_autoencoder_val, X_autoencoder_val))\n",
    "    \n",
    "    encoded_train_logreg = encoder.predict(X_train_logreg)\n",
    "    encoded_train_logreg = pd.DataFrame(encoded_train_logreg[0])\n",
    "\n",
    "    encoded_val_logreg = encoder.predict(X_val_logreg)\n",
    "    encoded_val_logreg = pd.DataFrame(encoded_val_logreg[0])\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear', penalty=\"l1\", C=1, multi_class=\"auto\").fit(encoded_train_logreg, y_train_logreg)\n",
    "    \n",
    "    history_df = pd.DataFrame(fit_hist.history)\n",
    "    score = clf.score(encoded_val_logreg, y_val_logreg)\n",
    "    conf_matrix = confusion_matrix(y_val_logreg, clf.predict(encoded_val_logreg))\n",
    "    \n",
    "    return history_df, score, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for the whole model - 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 of 5\n",
      "Train on 9908 samples, validate on 1101 samples\n",
      "Epoch 1/150\n",
      " - 31s - loss: 10379.9346 - val_loss: 10194.8759\n",
      "Epoch 2/150\n",
      " - 28s - loss: 10038.9273 - val_loss: 10070.5314\n",
      "Epoch 3/150\n",
      " - 27s - loss: 9987.0178 - val_loss: 10009.2176\n",
      "Epoch 4/150\n",
      " - 28s - loss: 9957.1261 - val_loss: 9996.7418\n",
      "Epoch 5/150\n",
      " - 27s - loss: 9938.0894 - val_loss: 9980.0530\n",
      "Epoch 6/150\n",
      " - 28s - loss: 9925.5694 - val_loss: 9965.4989\n",
      "Epoch 7/150\n",
      " - 28s - loss: 9916.8586 - val_loss: 9956.3913\n",
      "Epoch 8/150\n"
     ]
    }
   ],
   "source": [
    "history = {}\n",
    "scores = []\n",
    "confusion_matrixes = []\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "i=1\n",
    "\n",
    "for train_index, test_index in skf.split(X_brca_train, y_brca_train):\n",
    "    print('Fold {} of {}'.format(i, skf.n_splits))\n",
    "    \n",
    "    X_train, X_val = X_brca_train.iloc[train_index], X_brca_train.iloc[test_index]\n",
    "    y_train, y_val = y_brca_train.iloc[train_index], y_brca_train.iloc[test_index]\n",
    "    \n",
    "    # Prepare data to train Variational Autoencoder (merge dataframes and normalize)\n",
    "    X_autoencoder = pd.concat([X_train, X_tcga_no_brca], sort=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_autoencoder)\n",
    "    X_autoencoder_scaled = pd.DataFrame(scaler.transform(X_autoencoder), columns=X_autoencoder.columns)\n",
    "\n",
    "    # Scale logistic regression data\n",
    "    scaler.fit(X_brca_train)\n",
    "    X_brca_train_scaled = pd.DataFrame(scaler.transform(X_brca_train), columns=X_brca_train.columns)\n",
    "    X_brca_test_scaled = pd.DataFrame(scaler.transform(X_brca_test), columns=X_brca_test.columns)\n",
    "    \n",
    "    \n",
    "    # Because it is used by the error function\n",
    "    original_dim = X_autoencoder.shape[1]\n",
    "    \n",
    "    #Train the Model\n",
    "    hist, score, conf_matrix = train_model(X_autoencoder=X_autoencoder_scaled, \n",
    "                                           X_train_logreg=X_train, \n",
    "                                           y_train_logreg=y_train, \n",
    "                                           X_val_logreg=X_val, \n",
    "                                           y_val_logreg=y_val,\n",
    "                                           intermediate_dim=100,\n",
    "                                           latent_dim=100)\n",
    "    \n",
    "    print(score)\n",
    "    history[i-1] = hist\n",
    "    scores.append(score)\n",
    "    confusion_matrixes.append(conf_matrix)\n",
    "    i+=1\n",
    "print('5-Fold results: {}'.format(scores))\n",
    "print('Latent dim: {}, Accuracy: {}'.format(latent_dim, np.mean(scores)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
